\chapter{Conclusion}
How should we design systems for analyzing and exploring the high-throughput
datasets that facilitate sharing, reuse, and reproducibility? This dissertation
shows that in many cases the solution is to decompose the applications into
small entities that communicate using open protocols. This enables the
development of unified systems for reproducible exploration and analysis. 

While high-throughput datasets and computing systems will undoubtedly evolve, we
believe that the \gls{sme} approach proposed here can offer a new perspective on
developing applications for exploring and analyzing biological data. We hope
that our approach can steer the development of bioinformatics applications away
from large monolithic applications to applications composed of diverse systems.
We believe that this approach can help the community develop new systems to
faster meet the needs of the upcoming biological dataset analyses. 

In Chapter \ref{biodata} we show an approach to store the biological data and
analysis code from a complex epidemiological study in a shareable software
package. We show how we explicitly track versions of code and data, and how we
can generate reproducible data analysis reports for the processed datasets.
We believe that future studies can benefit from applying our approach, and that
future advances in research is dependent on sharing of both datasets and
analysis code. 
% We also show the usefulness our approach as a basis for standardizing the
% preprocessing of its biological datasets. 
In chapter \ref{interactive} we show how we can build
interactive data exploration applications that interface with these software
packages through a microservice architecture. We have implemented this approach
through the microservices in \emph{Kvik}. We show that this architecture style
is suitable for building such applications, and have used it to develop the
\emph{Kvik Pathways} and \emph{MIxT} web applications. These have been
successfully used to explore transcriptional profiles in the \gls{nowac} study,
especially to investigate the interactions between genes and pathways in the
patient tumor and blood cells. 
We believe that the research community in general will benefit greatly if more
projects start to develop their applications using our approach. It simplifies
sharing of computational resources, and we believe that the future of research
will depend on collaborative efforts. 
In chapter \ref{pipeline} use the same approach, to compose systems of disparate
tools, for developing biological data
analysis pipelines, implemented in \texttt{walrus}. 
To ensure reproducible results, we supplement the processing
with data versioning to track provenance of the data through the pipeline and
across pipeline versions. We have used \texttt{walrus} in the clinical setting
to develop a \gls{wes} pipeline for discovering \glspl{snp}, genomic
variants, and somatic mutations, in a breast cancer patient's metastatic
lesion. 

Combined, these systems demonstrate the applicability of our approach across a
range of different use cases. 

In the rest of this chapter we summarize end-to-end lessons learned during this
work. We then discuss the work in the context of research and in the clinical
setting,
before we propose areas for future work. 


\section{Lessons Learned}
Through the design of the \gls{sme} approach for analyzing and exploring
biological datasets, as well as the different implementations of the approach,
we have solved challenges and learned some key lessons.

\textbf{There is no single solution programming language or system.} 
In the field of bioinformatics there have been tremendous efforts to develop
analysis tools for improving the analysis of new biological datasets.  This has
led to systems being written in a plethora of different languages, and deployed
on top of different systems. This is the main motivation behind our \gls{sme}
approach together with software containers.

\textbf{Take advantage of existing tools.} The ability to develop applications
for analyzing biological datasets comes from the availability of existing tools.
By developing easy-to-use interfaces for the existing tools, it is possible
to develop new applications without reimplementing key features. 

\textbf{Simplicity is key.} When proposing a new approach for either managing
datasets, writing data exploration applications, or developing analysis
pipelines, it is not possible to overstate the importance of the simplicity of
the solution. 

\textbf{Researchers are not software engineers.} 
When designing a new approach to store and analyze high-throughout biological
datasets, it is important to remember that its users have limited software
engineering backgrounds. Especially when the implementation is based on complex
systems such as \texttt{git}, the learning curve for the system is steep and
require training of its users. In our project we have organized workshops in
both R and git to get the researchers in the \gls{nowac} study comfortable with
these systems to follow our best practices. 

% \textbf{Define the data storage and analysis requirements before collecting
% data.} 

% \section{Broader Impact}
% balkanized 
% clinica

\section{Future Work}
As we have discussed in previous chapters, there are some limitations to our
approach and its implementations. To summarize these, the main areas for
improvement are: 

\begin{itemize} 

\item \textbf{Versioning of datasets:} \texttt{git} was not designed to version
large binary files, such as biological datasets, and it does not provide the
required performance or scalability to version the large biological data. 

% Rob Pike said this: 
% Measure. Don't tune for speed until you've measured, and even then don't
% unless one part of the code overwhelms the rest.
% http://www.catb.org/~esr/writings/taoup/html/ch01s06.html
\item \textbf{Additional evaluation:} while we have shown that the \gls{sme}
approach can be used to develop systems for managing research data, developing
interactive applications and data analysis pipelines, we would like to
better understand its performance and scalability. 

\item \textbf{Refactoring and test coverage:} while we provide fully implemented
solutions for data storage, interactive applications, and data analysis
pipelines, they all have areas of improvement with regards to performance,
scalability, and robustness. 

\item \textbf{Distributed execution:} while \texttt{walrus} orchestrate
execution of Docker containers, we do not support the execution of these on
multiple compute nodes. Distributing the computation on multiple machines will
reduce the execution time if we can share the data across the
machines efficiently. We would also like to evaluate the possibility of using an
existing container orchestration system, such as
Kubernetes, to orchestrate the execution of an analysis pipeline. Many of these
already provide functionality for distributed execution of software containers. 

\item \textbf{Wide adoption of a pipeline description format:} we are not the
first to propose a new computing standard.\footnote{\url{xkcd.com/927}} We found
that the current standards were either too verbose, e.g., \gls{cwl}, or did not
enforce the use of software containers. This led us to our own description
format, but we recognize the need for a single open standard, and hope to
contribute to its development. 

\end{itemize} 

We aim to refine and continue development on our \glspl{sme} approach to address
these challenges, and that we can inspire a more unified development community
in bioinformatics. We believe that the future of cancer research relies on the
successful integration of diverse data analysis and storage systems from
different research institutions.  This will definitely continue to be an
interesting area of research.


