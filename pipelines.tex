\chapter{Deep Analysis Pipelines}\label{pipeline}  
In this chapter we discuss our approach to analyzing high-throughput genomic
datasets through deep analysis pipelines, and its implementation in 
walrus.\cite{walrus} We also evaluate the performance of walrus and show its
usefulness in a precision medicine setting. While walrus was developed in this
context we also show its usefulness in other areas, specifically for
\gls{rna}-seq analyses. 

\section{Use Case and Motivation} 
Precision medicine uses patient-specific molecular information to diagnose and
categorize disease to tailor treatment to improve health
outcome.\cite{national2011toward} Important goals in precision medicine are to
learn about the variability of the molecular characteristics of individual
tumors, their relationship to outcome, and to improve diagnosis and
therapy.\cite{tannock2016limits} Cancer institutions are therefore now
offering dedicated personalized medicine programs. 

For cancer, high throughput sequencing is an emerging technology to facilitate
personalized diagnosis and treatment since it enables collecting high quality
genomic data from patients at a low cost. Data collection is becoming cheaper,
but the downstream computational analysis is still time-consuming and thereby
a costly part of the experiment.  This is because of the manual efforts to set
up, analyze, and maintain the analysis pipelines. These pipelines consist of
many steps that transform raw data into interpretable
results.\cite{diao2015building} These pipelines often consists of in-house or
third party tools and scripts that each transform input files and produce some
output. Although different tools exist, it is necessary to carefully explore
different tools and parameters to choose the most efficient to apply for a
dedicated question.\cite{servant2014bioinformatics} The complexity of the tools
vary from toolkits such as the \gls{gatk} to small custom \texttt{bash} or
\texttt{R} scripts.  In addition, some tools interface with databases whose
versions and content will impact the overall result.\cite{sboner2015primer}

Improperly developed analysis pipelines for precision medicine may generate
inaccurate results, which may have negative consequences for patient
care.\cite{roy2017standards} Users and clinicians therefore need systems that
can track pipeline tool versions, their input parameters, and data. Both to
thoroughly document what produced the final clinical reports, and to iteratively
improve the quality of the pipeline during development. Because of the
iterative process of developing the analysis pipeline, it is necessary to use
analysis tools that facilitate modifying pipeline steps and adding new ones with
little developer effort.

Developing a system that enables researchers to write and share reproducible
analysis pipelines will enable the scientific community to analyze
high-throughput genomic datasets faster and more unified. By combining
versioning of datasets and pipeline configurations, a pipeline management system
will provide interpretable and reproducible results long after the initial data
analysis will have completed. These features will together promote reproducible
science and improve the overall quality of the analyses. 

\subsection{Initial Data Analysis Pipeline} 
We have analyzed DNA sequence data from a breast cancer patient's
primary tumor and adjacent normal cells to identify the molecular signature of
the patient's tumor and germline. When the patient  later relapsed we analyzed
sequence data from the patient's metastasis to provide an extensive comparison
against the primary and to identify the molecular drivers of the patient's
tumor. 

We used \gls{wgs} to sequence the primary tumor and adjacent normal cells at an
average depth of 20, and \gls{wes} at an average depth of 300. The biological
samples were sequenced at the Genome Quebec Innovation Centre, and we stored the
raw datasets on our in-house server.  From the analysis pipelines we generated
reports with end results, such as detected somatic mutations, that was
distributed to both the patient and the treating oncologists. These could be
used to guide diagnosis and treatment, and give more detailed insight into both
the primary and metastasis.  When the patient relapsed we analyzed \gls{wes}
data using our own pipeline manager, \texttt{walrus}, to investigate the
metastasis and compare it to the primary tumor. 

For the initial \gls{wgs} analysis we developed a pipeline to investigate
somatic and germline mutations based on Broad Institute's best practices. We
developed the analysis pipeline on our in-house compute server using a
\emph{bash} script under version control with \emph{git} to track changes as we
developed the analysis pipeline. The pipeline consisted of tools including
picard\cite{picard}, fastqc\cite{fastqc}, trimmomatic\cite{trimmomatic}, and the
\gls{gatk}.\cite{gatk} While the analysis tools themselves provide the necessary
functionality to give insights in the disease, 
% LAB: Kan vi gi ett illustrerende eksempel p√• hvorfor dette var viktig i
% "motivating use case". Ellers blir det rart hvis hovedpoenget med walrus ikke
% var motivert.
ensuring that the analyses could be fully reproduced later left areas in need of
improvement.

We chose a command-line script over more complex pipelining tools or workbenches
such as Galaxy\cite{goecks2010galaxy} because of its fast setup time on our
available compute infrastructure, and familiar interface. More complex systems
could be beneficial in larger research groups with more resources to compute
infrastructure maintenance, whereas command-line scripting languages require
little infrastructure maintenance over normal use. In addition, while there are
off-site solutions for executing scientific workflows, analyzing sensitive data
often put hard restrictions on where the data can be stored and analyzed.

After we completed the first round of analyses we summarized our efforts and
noted features that pipeline management systems should satisfy: 
\begin{itemize}
    \item  Datasets and databases should be under version control and stored
        along with the pipeline description. In the analysis script we
        referenced to datasets and databases by their physical location on a
        storage system, but these were later moved without updating the pipeline
        description causing extra work. A solution would be to add the data to
        the same version control repository hosting the pipeline description.
    \item The specific pipeline tools should also be kept available for
        later use. Often in bioinformatics, just installing a tool is a
        time-consuming process because of their many dependencies. 
    \item It should be easy to add new tools to an existing
        pipeline and execution environment. This includes installing the specific
        tool and adding to an existing pipeline. Bundling tools within software
        containers, such as Docker, and hosting them on an online registry
        simplifies the tool installation process since the only requirement is
        the container runtime.
    \item While bash scripts have their
        limitations, using a well-known format that closely resembles the normal
        command-line use clearly have its advantages. It is easy to understand
        what tools were used, their input parameters, and the data flow.
        However, from our experience when these analysis scripts grow too large
        they become too complex to modify and maintain. 
    \item While there are new and promising state-of-the art pipeline
        managers, many of these also require state-of-the-art computing
        infrastructure to run. This may not be the case at cancer research
        and clinical institutions. 
\end{itemize} 

The above problem areas are not just applicable to our research group, but
common to other research and precision medicine projects as well. Especially
when hospitals and research groups aim to apply personalized medicine efforts to
guide therapeutic strategies and diagnosis, the analyses will have to be able to
be easily reproducible later. We used the lessons learned to  design and
implement \texttt{walrus}, a command line tool for developing and running data
analysis pipelines. It automatically orchestrates the execution of different
tools, and tracks tool versions and parameters, as well as datasets through the
analysis pipeline. It provides users a simple interface to inspect differences
in pipeline runs, and retrieve previous analysis results and configurations. In
the remainder of the paper we describe the design and implementation of
\texttt{walrus}, its clinical use, its performance, and how it relates to other
pipeline managers. 

\section{\texttt{walrus}} 
\texttt{walrus} is a tool for developing and executing data analysis pipelines.
It stores information about tool versions, tool parameters, input data,
intermediate data, output data, as well as execution environments to simplify
the process of reproducing data analyses. Users write descriptions of their
analysis pipelines using a familiar syntax and \texttt{walrus}  uses this
description to orchestrate the execution of the pipeline. In \texttt{walrus}  we
package all tools in software containers to capture the details of the different
execution environments. While we have used \texttt{walrus} to analyze
high-throughput datasets in precision medicine, it is a general tool that can
analyze any type of data, e.g. image datasets for machine learning. It has few
dependencies and runs on any platform that supports Docker containers. While
other popular pipeline managers require the use of cluster computers or cloud
environment, we focus on single compute nodes often found in clinical
environments such as hospitals. 

\texttt{walrus} is implemented as a command-line tool in the Go programming
language. We use the popular software container implementation
Docker\cite{docker} to provide reproducible execution environments, and
interface with git together with \texttt{git-lfs}\cite{gitlfs} to version control
datasets and pipeline descriptions. By choosing Docker and git we have built a
tool that easily integrates with current bioinformatic tools and workflows. It
runs both natively or within its own Docker container to simplify its
installation process.

With \texttt{walrus} we target pipeline developers that use command-line tools
and scripting languages to build and run analysis pipelines. Users can use
existing Docker containers from sources such as
BioContainers\cite{biocontainers} or build containers with their own tools.  We
integrate with the current workflow using git to version control analysis
scripts, and use \texttt{git-lfs} for versioning of datasets as well. We have
designed the pipeline description format resembles the command line syntax as
much as possible. This is one of the major strengths of walrus. It uses a
familiar syntax and format, and does not require the users to explicitly
declare which files in the pipeline to version control. 


\subsection{Pipeline Configuration}
Users configure analysis pipelines by writing pipeline description files in a
human readable format such as \gls{json} or \gls{yaml}. A pipeline description
contains a list of stages, each with inputs and outputs, along with optional
information such as comments or configuration parameters such as caching rules
for intermediate results. Listing \ref{examplelisting} shows an example pipeline
stage that uses MuTect\cite{mutect} to detect somatic point mutations. Users
can also specify the tool versions by selecting a specific Docker image, for
example using MuTect version 1.1.7 as in Listing \ref{examplelisting}, line 3. 

Users specify the flow of data in the pipeline within the pipeline description,
as well as the dependencies between the steps. Since pipeline configurations can
become complex, users can view their pipelines using an interactive web-based
tool, or export their pipeline as a DOT file for visualization in tools such as
Graphviz.\cite{ellson2001graphviz}

\begin{lstlisting}[caption={Example pipeline stage for a tool that detects
somatic point mutations. It reads a reference sequence file together with both
tumor and normal sequences, and produces an output file with the detected
mutations.},
label={examplelisting}, 
basicstyle=\ttfamily\scriptsize]
     {
       "Name": "mutect",
       "Image": "fjukstad/mutect:1.1.7",
       "Cmd": [
         "--analysis_type","MuTect",
         "--reference_sequence","/walrus/input/reference.fasta",
         "--input_file:normal","/walrus/input/normal.bam",
         "--input_file:tumor","/walrus/input/tumor.bam",
         "-L","/walrus/input/targets.bed",
         "--out","/walrus/mutect/mutect-stats-txt",
         "--vcf","/walrus/mutect/mutect.vcf"
       ],
       "Inputs":[
          "input" 
       ]
     }
\end{lstlisting}

Users add data to an analysis pipeline by specifying the location of the
input data in the pipeline description, and \texttt{walrus} automatically mounts
it to the container running the analysis. The location of the input files can
either be local or remote locations such as an FTP server. When the pipeline is
completed, \texttt{walrus} will store all the input, intermediate and output
data to a user-specified location which is under version control.

\subsection{Pipeline Execution}
When users have written a pipeline description for their analyses, they can use
the command-line interface of \texttt{walrus} to run the analysis pipeline.
\texttt{walrus} builds an execution plan from the pipeline description and runs
it for the user. It uses the input and output fields of each pipeline stage to
construct a \gls{dag} where each node is a pipeline stage and the links are
input/output data to the stages. From this graph \texttt{walrus} can determine
parallelizable stages and coordinate the execution of the pipeline. 

In \texttt{walrus}, each pipeline stage is run in a separate container, and
users can specify container versions in the pipeline description to specify the
correct version of a tool. We treat a container as a single executable and users
specify tool input arguments in the pipeline description file using standard
command line syntax. \texttt{walrus} will automatically build or download the
container images with the analysis tools, and start these with the user-defined
input parameters and mount the appropriate input datasets. While the pipeline is
running, \texttt{walrus} monitors running stages and schedules the execution of
subsequent pipeline stages when their respective input data become available. We
have designed \texttt{walrus} to execute an analysis pipeline on a single large
server, but since the tools are run within containers, these can easily be
orchestrated across a range of servers in future versions. 

Users can select from containers pre-installed with bioinformatics tools, or build
their own using a standard Dockerfile. Through software containers
\texttt{walrus} can provide a reproducible execution environment for the
pipeline, and containers provide simple execution on a wide range of software
and hardware platforms.  With initiatives such as
BioContainers, researchers can make use of already existing
containers without having to re-write their own. Data in each pipeline step is
automatically mounted and made available within each Docker container. By simply
relying on Docker \texttt{walrus} requires little software setup to run
different bioinformatics tools. 

While \texttt{walrus} executes a single pipeline on one physical server, it
supports both data and tool parallelism, as well as any parallelization
strategies within each tool, e.g. multi-threading. To enable data and tool
parallelism, e.g. run the same analyses to analyse a set of samples, users list
the samples in the pipeline description and \texttt{walrus} will automatically
run each sample through the pipeline in parallel. While we can parallelize the
independent pipeline steps, the performance of an analysis pipeline relies on
each of the independent tools and available compute power. Techniques such as
multithreading can improve the performance of a tool, and \texttt{walrus} users
can make use of these techniques if their are available through the command line
interfaces of the tools.

Upon successful completion of a pipeline run, \texttt{walrus} will write a
verbose pipeline description file to the output directory. This file contains
information on the runtime of each step, which steps were parallelized, and
provenance related information to the output data from each step. Users can
investigate this file to get a more detailed look on the completed pipeline. In
addition to this output file \texttt{walrus} will return a unique version ID for
the pipeline run, which later can be used to investigate a previous pipeline
run.


\subsection{Data Management}
In \texttt{walrus} we provide an interface for users to track their analysis
data through a version control system. 
This allows users to inspect data from previous pipeline runs without having to
recompute all the data. \texttt{walrus} stores all intermediate and output data
in an output directory specified by the user, which is under version control
automatically by \texttt{walrus} when new data is produced by the pipeline. We
track changes at file granularity. 

In \texttt{walrus} we interface with \texttt{git} to track any output file from
the analysis pipeline. When users execute a pipeline, \texttt{walrus} will
automatically add and commit output data to a git repository using
\texttt{git-lfs}.  Users typically use a single repository per pipeline, but can
share the same repository to version multiple pipelines as well. With
\texttt{git-lfs}, instead of writing large blobs to a repository it writes small
pointer files that contains the hash of the original file, the size of the file,
and the version of\texttt{git-lfs} used. The files themselves are stored
separately which makes the size of the repository small and manageable with git.
Once \texttt{walrus} has started to track output datasets, users can use regular
git commands to inspect its version history. 
The main reason why we chose git and \texttt{git-lfs} for version control is
that git is the de facto standard for versioning source code, and we want to
include versioning of datasets without altering the typical development
workflow. 

Since we are working with potentially sensitive datasets \texttt{walrus} is
targeted at users that use a local compute and storage servers. It is up to
users to configure a remote tracker for their repositories, but we provide
command-line functionality in \texttt{walrus} to run a \texttt{git-lfs} server
that can store users' contents.  They can use their default remotes, such as
Github, for hosting source code, but they must themselves provide the remote
server to host their data.

\subsection{Pipeline Reconfiguration and Re-execution}
Reconfiguring a pipeline is common practice in precision medicine, e.g. to
ensure that genomic variants are called with a desired sensitivity and
specificity.  To reconfigure an existing pipeline users make the applicable
changes to the pipeline description and re-run it with \texttt{walrus}.
\texttt{walrus} will then recompute the necessary steps and return a version ID
for the newly run pipeline. This ID can be used to compare pipeline runs, the
changes made, and optionally restore the data and configuration from a
previous run.  Reconfiguring the pipeline to use updated tools or reference
genomes will alter the pipeline configuration and force \texttt{walrus} to
recompute the applicable pipeline stages. 

% LAB: Hva med intermediate files?
The command-line interface of \texttt{walrus} provides functionality to restore
results from a previous run, as well as printing information about a completed
pipeline.  To restore a previous pipeline run, users use the \texttt{restore}
command line flag in \texttt{walrus} together with the version ID of the
respective pipeline run. \texttt{walrus} will interface with git to restore the
files to their state at the necessary point in time.

\section{Results}
To evaluate the usefulness of \texttt{walrus} we demonstrate its use in a
clinical setting, and the low computational time and storage overhead to support
reproducible analyses.

\subsection{Clinical Application} 
We have used \texttt{walrus} to analyze a whole-exome data from a sample
in the McGill Genome Quebec [MGGQ] dataset (GSE58644)\cite{tofigh2014prognostic}
to discover \glspl{snp}, genomic variants and somatic mutations. We
interactively developed a pipeline description that follows the best-practices
of The Broad Institute\footnote{Online at
\url{software.broadinstitute.org/gatk/best-practices}.} and generated reports
that summarized the findings to share the results. Figure \ref{webshotfig} shows
a screenshot from the web-based visualization in \texttt{walrus} of the
pipeline. 

\begin{figure}
    \centering
\includegraphics[width=10cm]{figures/webshot.png}
    \caption[Screenshot of the web-based visualization in
    \texttt{walrus}]{Screenshot of the web-based visualization in
    \texttt{walrus}. The user has zoomed in to inspect the pipeline step which
    marks duplicate reads in the tumor sequence data.}
    \label{webshotfig}
\end{figure} 

From the analyses we discovered inherited germline mutations that are recognized
to be among the top 50 mutations associated with an increased risk of familial
breast cancer. We also discovered a germline deletion which has been associated
with an increased risk of breast cancer. We also discovered mutations in a
specific gene that might explain why specific drug had not been effective in the
treatment of the primary tumor. From the profile of the primary tumor we
discovered many somatic events (around 30 000) across the whole genome with
about 1000 in coding regions, and 500 of these were coding for non-synonymous
mutations.  We did not see amplification or constituent activation of growth
factors like HER2, EGFR or other players in breast cancer. Because of the
germline mutation, early recurrence, and lack of DNA events, we suspect that the
patient's primary tumor was highly immunogenic. We have also identified several
mutations and copy number changes in key driver genes. This includes a mutation
in a gene that creates a premature stop codon, truncating one copy of the gene.

While we cannot share the results in details or the sensitive dataset, we have
made the pipeline description available at \url{github.com/uit-bdps/walrus}
along with other example pipelines. 

\subsection{Example Dataset}
To demonstrate the performance of \texttt{walrus} and the ability to track and
detect changes in an analysis pipeline, we have implemented one of the variant
calling pipelines from \cite{cornish2015comparison} using tools from picard and
the \gls{gatk}. We show the storage and computational overhead of our approach,
and the benefit of capturing the pipeline specification using a pipeline
manager.  The pipeline description and code is available along with
\texttt{walrus} at \url{github.com/uit-bdps/walrus}. Figure
\ref{benchpipefigure} shows a simple graphical representation of the pipeline. 

\begin{figure}
    \centering
\includegraphics[width=\linewidth]{figures/graph.pdf}
    \caption[DOT representations of a pipeline in \texttt{walrus}]{In addition
    to the web-based inteactive pipeline visualization, \texttt{walrus} can also
    generate DOT representations of pipelines. The figure shows the example
    variant calling pipeline we used in the performance evaluation.} 
    \label{benchpipefigure}
\end{figure} 

\subsection{Performance and Resource Usage}
We first run the variant calling pipeline without any additional provenance
tracking or storing of output or intermediate datasets. This is to get a
baseline performance measurement for how long we expect the pipeline to run. We
then run a second experiment to measure the overhead of versioning output and
intermediate data. Then we introduce a parameter change in one of the pipeline
steps which results in new intermediate and output datasets. Specifically we
change the \texttt{--maxReadsForRealignment} parameter in the indel realigner
step back to its default (See the online pipeline description for more details).
This forces \texttt{walrus} to recompute the indel realigner step and any
subsequent steps.  To illustrate how \texttt{walrus} can restore old pipeline
configurations and results, we restore the pipeline to the initial configuration
and results. We show the computational overhead and storage usage of restoring a
previous pipeline configuration. 

Reproducing results from a scientific publication can be a difficult task. For
example, because the rendering of the online version of the pipeline in
\cite{cornish2015comparison} converts two consecutive hypens (\texttt{--})
into single em dashes (---), the pipeline will not run using the specified input
parameters. However, PDF versions of the paper lists the parameters correctly.
In addition, the input filenames in the variant calling step do not correspond
to any output files in previous steps, but because of their similarity to
previous output files we assume that this is just a typo.  These issues in
addition to missing commands for e.g. the filtering step highlights the clear
benefit of writing and reporting the analysis pipeline using a tool such as
\texttt{walrus}. 

Table \ref{resultstable} shows the runtime and storage use of the different
experiments. In the second experiment we can see the added overhead of adding
version control to the dataset. In total, an hour is added to the runtime and
the data size is doubled. The doubling comes from git-lfs hard copying the data
into a subdirectory of the \texttt{.git} folder in the repository. With git-lfs
users can move all datasets to a remote server reducing the local storage
requirements. 
In the third experiment we can see that only the downstream analyses from
configuring the indel realignment parameter is executed. It generates 30GB of
additional data, but the execution time is limited to the applicable stages.
Restoring the pipeline to a previous configuration is almost instantaneous since
the data is already available locally and git only has to modify the pointers to
the correct files in the \texttt{.git} subdirectory. 

\begin{table}[ht!]
    \centering
    \caption{Runtime and storage use of the example variant-calling pipeline
    developed with \texttt{walrus}.} 
    \begin{tabular}{ | c | p{4cm} | p{2cm} | p{2cm} |}
    \hline
    Experiment & Task & Runtime & Storage Use \\ \hline
    1 & Run pipeline with default configuration & 21 hours 50 minutes & 235 GB
        \\ \hline
    2 & Run the default pipeline with version control of data & 23 hours 9
        minutes & 470 GB \\ \hline
    3 & Re-run the pipeline with modified indel realignment parameter & 13 hours
        & 500 GB \\ \hline
    4 & Restoring pipeline back to the default configuration & < 1 second &
        500GB \\ \hline
%    5 & Examining the pipeline output & TBD & 500GB \\ \hline 
    \end{tabular}
    \label{resultstable}
\end{table}


% \subsubsection{Provenance} 
% Listing \ref{provlist} shows the output of task 5, examining the pipeline output
% against previous versions. It shows how the subsequent pipeline steps have been
% re-run and that the pipeline description was changed. 


\section{Related Work} 
There are a wealth of pipeline specification formats and workflow managers
available. Some are targeted at users with programming experience while others
provide simple \glspl{gui}. 

We have previously conducted a survey of different
specialized bioinformatics pipelines.\cite{fjukstad2017review} 
The pipelines were selected to show how analysis pipelines for different
applications use different technologies for configuring, executing and storing
intermediate and output data. In the review, we targeted specialized analysis
pipelines that support scaling out the pipelines to run on \gls{hpc} or cloud
computing platforms. 

Here we describe general systems for developing data analysis
pipelines, not just specialized bioinformatics pipelines. While most provide
viable options for genomic analyses, we have found most to complex to install
and maintain in clinical settings. We discuss tools that use the common
\gls{cwl} pipeline specification and systems that provide versioning of data. 

\gls{cwl} is a specification for describing analysis workflows and
tools.\cite{cwl} A pipeline is written as a \gls{json} or \gls{yaml} file,
or a mix of the two, and describes each step in detail, e.g. what tool to run,
its input parameters, input data and output data. The pipeline descriptions
are text files that can be under version control and shared between projects. There
are multiple implementations of \gls{cwl} workflow platforms, e.g. the reference
implementation cwl\_runner\cite{cwl}, Arvados\cite{arvados}, Rabix\cite{rabix},
Toil\cite{toil}, Galaxy\cite{goecks2010galaxy}, and AWE.\cite{awe} It is no
requirement to run tools within containers, but implementations can support it.
There are few of these tools that support versioning of the data.  Galaxy is an
open web-based platform for reproducible analysis of large high-throughput
datasets.\cite{goecks2010galaxy} It is possible to run Galaxy on local compute
clusters, in the cloud, or using the online Galaxy site.\footnote{Available at
\url{usegalaxy.org}.} In Galaxy users set up an analysis pipeline using a
web-based graphical interface, and it is also possible to export or import an
existing workflow to an \gls{xml} file.\footnote{An alpha version of Galaxy with
\gls{cwl} support is available at
\url{github.com/common-workflow-language/galaxy}.}  We chose not to use Galaxy
because of missing command-line and scripting support, along with little support
for running workflows with different configurations.\cite{spjuth2015experiences}
Rabix provides checksums of output data to verify it against the actual output
from the pipeline. This is similar to the checksums found in the git-lfs pointer
files, but they do not store the original files for later. An interesting
project that uses CWL in production is The Cancer Genomics
Cloud\cite{lau2017cancer}. They currently support CWL version 1.0 and are
planning on integrating Rabix as its CWL executor. 
Arvados stores the data in a distributed storage system, Keep, that provides
both storage and versioning of data. We chose not to use \gls{cwl} and its
implementations because of its relaxed restrictions on having to use containers,
its verbose pipeline descriptions, and the complex compute architecture required
for some implementations. We are however experimenting with an extension to
\texttt{walrus} that translates pipeline descriptions written in \texttt{walrus}
to \gls{cwl} pipeline descriptions. 

Pachyderm is a system for running  big data analysis pipelines. It provides
complete version control for data and leverages the container ecosystem to
provide reproducible data processing.\cite{pachyderm} Pachyderm consists of a
file system (\gls{pfs}) and a processing system (\gls{pps}).  \gls{pfs} is a
file system with git-like semantics for storing data used in data analysis
pipelines. Pachyderm ensures complete analysis reproducibility by providing
version control for datasets in addition to the containerized execution
environments. Both \gls{pfs} and \gls{pps} is implemented on top of
Kubernetes.\cite{kubernetes} There are now recent efforts to develop
bioinformatics workflows with Pachyderm that show great
promise. In \cite{doi:10.1093/bioinformatics/bty699}, the authors show the
potential performance improvements of single workflow steps, not the full
pipeline, when executing a pipeline in Pachyderm. They unfortunately do not show
the time to import data into \gls{pfs}, run the full pipeline, and optionally
investigate different versions of the intermediate, or output datasets. 

We believe that the approach in Pachyderm with
version controlling datasets and containerizing each pipeline step is, along
with walrus, the correct approach to truly reproducible data analysis pipelines.
The reason we did not use Kubernetes and Pachyderm was because our compute
infrastructure did not support it. In addition, we did not want to use a
separate tool, \gls{pfs}, for data versioning, we wanted to integrate it with
our current practice of using git for versioning.  

Snakemake is a long-running project for analyzing bioinformatic
datasets.\cite{koster2012snakemake} It uses a Python-based language to describe
pipelines, similar to the familiar Makefile syntax, and can execute these
pipelines on local machines, compute clusters or in the cloud. To ensure
reproducible workflows, Snakemake integrates with Bioconda to provide the
correct versions of the different tools used in the workflows. It integrates
with Docker and Singularity containers\cite{singularity} to provide isolated
execution, and in later versions Snakemake allows pipeline execution on a
Kubernetes cluster. Because Snakemake did not provide necessary integration with
software containers at the time we developing our analysis pipeline, we did not
find it to be a viable alternative. For example, support for pipelines
consisting of Docker containers pre-installed with bioinformatics tools came a
year later than walrus. 

Another alternative to develop analysis pipelines is
Nextflow.\cite{di2017nextflow} Nextflow uses its own language to describe
analysis pipelines and supports execution within Docker and Singularity
containers. 

As discussed in \cite{NIK, fjukstad2017review}, recent projects propose to use
containers for life science research. The BioContainers and
Bioboxes\cite{belmann2015bioboxes} projects address the challenge of installing
bioinformatics data analysis tools by maintaining a repository of Docker
containers for commonly used data analysis tools. Docker containers are shown to
have better than, or equal performance as \glspl{vm}.\cite{di2015impact} Both
forms of virtualization techniques introduce overhead in I/O-intensive
workloads, especially in \glspl{vm}, but introduce negligible CPU and memory
overhead. For precision medicine pipelines the overhead of Docker containers
will be negligible since these tend to be compute-intensive and they typically
run for several hours.\cite{di2015impact} Containers have also been proposed as
a solution to improve experiment reproducibility, by ensuring that the data
analysis tools are installed with the same
responsibilities.\cite{boettiger2015introduction} 

\section{Discussion}
Precision medicine requires flexible analysis pipelines that allow researchers
to explore different tools and parameters to analyze their data.  While there
are best practices to develop analysis pipelines for genomic datasets, e.g. to
discover genomic variants, there is still no de-facto standard for sharing the
detailed descriptions to simplify re-using and reproducing existing work.
Pipelines typically need to be tailored to fit each project and patient, and
different patients will typically elicit different molecular patterns that
require individual investigation. While we could follow the best practices to
develop our pipeline we explored different tools and parameters before we
arrived at the final analysis pipeline.  For example, in our \gls{wes} pipeline
we ran several rounds of preprocessing (trimming reads and quality control)
before we were sure that the data was ready for analysis. Having a pipeline
system that could keep track of different intermediate datasets, along with the
pipeline specification, simplifies the task of comparing the results from
pipeline tools and input parameters. While we have developed one approach to
version control genomic datasets in an analysis pipeline, we believe that there
is still room for improvement. 

% Significance in bioinformatics
% - general tool for any application
% - usable ideas for other tools that use kube etc.
% - only tool that integrates with git....... 

% clinical setting 
% - walrus will ensure repeatability of the computational analyses
% - enable analysis for researchers with simple compute infrastructure 
% - easy to set up

Unlike other proposed solutions for executing data analysis pipelines, walrus is
the only system we have discovered that explicitly uses git, and git-lfs, to
store output datasets. Other systems either use a specialized storage system, or
ignore data versioning at all. We believe that using a system that
bioinformaticians already use for source control management is simplest way to
allow users version their data along-side their analysis code. 

walrus is a very simple tool to set up. Since we only target users with single
large compute nodes, walrus can run within a Docker container making docker its
only dependency. Systems such as Nextflow, Galaxy or Pachyderm all require users
to set up complex compute infrastructures. 

While we provide one approach to version control datasets, there are still some
drawbacks. \texttt{git-lfs} supports large files, but in our results it added
5\% in runtime.  This makes the entire analysis pipeline slower, but we argue
that having the files under version control outweigh the runtime. In addition,
there are only a few public \texttt{gif-lfs} hosting platforms for datasets
larger than a few gigabytes, making it necessary to host these in-house.
In-house hosting may also be a requirement at different medical institutions.  

We aim to investigate the performance of running analysis pipelines with
\texttt{walrus}, and the potential benefit of its built-in data parallelism.
While our \gls{wes} analysis pipeline successfully run steps in parallel for the
tumor and adjacent normal tissue, we have not demonstrated the benefit
of doing so. This includes benchmarking and analyzing the system requirements
for doing precision medicine analyses.  We are also planning on exploring
parallelism strategies where we can split an input dataset into chromosomes and
run some steps in parallel for each chromosome, before merging the data again. 

We believe that future data analysis systems for precision medicine will follow
the lines of our proposed approach. Software container solutions provide
valuable information in the reporting of the analyses, and they impose little
performance overhead. Further, the development of container orchestration
systems such as Kubernetes is getting wide adoption nowadays, especially in
web-scale internet companies. However, the adoption of such systems in a
clinical setting depend on support from more tools, and also the addition
of new compute infrastructure. 

\section{Conclusions} 
We have designed and implemented \texttt{walrus}, a tool for developing 
reproducible data analysis pipelines for use in precision medicine. Precision
medicine requires that analyses are run on hospital compute infrastructures and
results are fully reproducible. By packaging analysis tools in software
containers, and tracking both intermediate and output data, \texttt{walrus}
provides the foundation for reproducible data analyses in the clinical setting.
We have used \texttt{walrus} to analyze a patient's metastatic lesions and
adjacent normal tissue to provide insights and recommendations for  cancer
treatment. 
