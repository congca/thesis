% observation
There is a rapid growth in the number of available biological datasets due to
the decreaseing cost of data collection. This brings opportunities to gain novel
insights to the underlying biological mechanisms in the development and
progression of diseases such as cancer, possibly leading to the development of
novel diagnostic tests or drugs for treatment.  The wide range of different
biological datasets has led to the development of hundreds of software packages
and systems to explore and analyze these datasets.  However, there are few
systems that are designed with the full analysis process in mind, from raw data
into interpretable and reproducible results.  While existing systems are used to
provide novel insights in diseases, there is little emphasis on reporting and
sharing information about its tools, their versions, input parameters, and other
information that can help others use the same known methods on their own
datasets. This leads to unnecessary difficulties to reuse known methods, and
reproducing the analyses, which leads to a longer analysis process and therefore
unrealized potential for scientific insights.

% challenges
There are four main challenges for researchers to analyze
and explore biological datasets. These challenges are common for large datasets
such as high-throughput sequencing data that require long-running, deep analysis
pipelines, as well as smaller datasets, such as microarray data, that require
complex, but short-running analysis pipelines. The first is the time and
knowledge required to find and set up the necessary analysis tools to start
analyzing a modern biological dataset. The second is recording full provenance
in the data exploration process. This includes ensuring the correct input
parameters, tool versions, database versions, and dataset versions when
analyzing, and reporting analysis results to enable reproducible science. A
third challenge is efficiently exploring the results of an analysis
interactively. This includes developing tools that can efficiently visualize the
heterogeneous resulting datsets and integrate them with known biology from
online databases. The final challenge is reusing the analysis pipelines and
exploration tools on new datasets and research questions. 

% existing tools and approaches
As a result, there are a wealth of specialized approaches and systems to analyze
modern biological data. Systems such as Galaxy\cite{galaxy} provide simple
\glspl{gui} to set up and run analysis pipelines. However it is difficult to
install and maintain, and less flexible for explorative analyses where it is
necessary to try out new tools and different tool
configurations.\cite{spjuth2015experiences} With R and its popular package
repository Bioconductor researchers can select from a wide range of packages to
tailor their analyses. These provide flexible environments but makes it
necessary for the analyst to manually record information about data, tools, and
tool versions. Systems such as Pachyderm\cite{pachyderm} or the \gls{cwl} and
its different implementations, can help users with a standardized way of
describing and sharing analysis pipelines.  However, many of these require
complex compute infrastructure and are too cumbersome to set up.  Shiny and
OpenCPU provide frameworks for application developers to build systems to
interactively explore results from statistical analyses.  These are useful to
build exploration applications that integrate with statistical analyses.  
With the addition of new datasets and methods every year, it seems that analysis
of biological data requires a wide array of different tools and systems.

% our solution
This dissertation argues that, instead, we can facilitate development of
reproducible analyses and data exploration systems of high-throughput biological
data thorugh the integration of disperate systems and data. In particular, we
show how software container technologies together with well-defined interfaces,
configurations, and orchestration provide the necessary foundation for these
systems. This allows for easy development and sharing of specialized analysis
systems. 

The resulting approach, which we have called Small Modular Entities, has several
key advantages when implementing systems to analyze and explore biological data:
\begin{itemize} 
    \item It enables applications to use tools written in any programming
        language, using open standards to communicate between tools and systems.

    \item It enables reproducible research by packaging applications
        and tools within containerized environments. This enables sharing of
        tools and ensures correct versions. 

    \item Through software container technology it becomes a simple task to
        deploy and scale up such applications. 
        
    \item It simplifies the sharing of analysis pipelines and workflows across
        different research teams and systems. This reduces the
        time-to-interpretation for biological datasets. 
\end{itemize} 

In collaboration with researchers in systems epidemiology and precision
medicine we developed a set of applications and systems necessary to organize,
analyze, and interpret their datasets. From these systems we extraplolated a set
of general design principles to form a unified approach. We evaluate this
approach through these systems using real datasets and show its viability. 

From a longer-term perspective we discuss the general patterns for implementing
reproducible data analysis systems for use in biomedical research. As more
datasets are produced every year, research will depend on the simplicity of the
systems for analyzing these, and that they provide the necessary functionality
to reproduce and share the analysis pipelines. 

\emph{Thesis statement}:
A unified development model based on software container infrastructure can
efficiently provide reproducible and easy to use environments to develop
applications for exploring and analyzing biological datasets. 

\section{Problems with Data Analysis and Exploration in Bioinformatics}
Today there is a move towards using more sophisticated approaches to analyze
biological datasets through workflow and pipeline mangers such as
Snakemake\cite{koster2012snakemake}, and the different implementations of the
\gls{cwl}\cite{cwl} such as Galaxy\cite{galaxy}. These simplify setting up and
executing the analysis pipeline pipeline. However, these tools still have their
limitations, such as maintenance and tool updates, and shell scripts are still
the de facto standard building analysis pipelines in bioinformatics. This comes
from the familiarity of the shell environment and the \gls{cli} of the different
tools.  For exploring biological data there are a range of tools, such as
Cytoscape\cite{cytoscape} and Circos\cite{circos}, that support importing an
already-analyzed dataset to visualize and browse the data. One problem with
these is that they are decoupled from the analysis, making it difficult to
retrace the underlying analyses. 

Although there are efforts to develop tools to help researchers explore and
analyze biological datasets, they current tools have several drawbacks:

\begin{enumerate}
    \item \textbf{Reusability:} Data exploration tools are often
        developed as a single specialized application, making it difficult to
        reuse parts of the application for other analyses or datasets. This
        leads to duplicate development effort and abandoned projects. 
    \item \textbf{Decoupling:} Data exploration tools are often decoupled from
        the statistical analyses. This often makes it a difficult to document
        and retrace the analyses behind the results. 
    \item \textbf{Complexity:} 
        Analyses that start as a simple script quickly become more complex to
        maintain and develop as developers add new functionality to the
        analyses.
        % LAB: kan stjele argumentasjon fra ADAM paperet der de kritiserer
        % "monolothic" bio applikasjoner. Eller fra Tanenbaum vs Linus.
    \item \textbf{Reproducibility:} While there are tools for analyzing most
        data types today, there is little or no effort to fully document the
        entire analysis process from raw data to interpretable results. This
        includes tool versions, parameters, data, and databases. This makes
        analysis results difficult to reproduce. 
\end{enumerate} 

Because of these drawbacks, a unified approach for reproducible data analysis
and exploration would have significant benefits to reduce the
time-to-interpretation of biological datasets. 

\section{Small Modular Entities} 
In collaboration with researchers in systems epidemiology and biology we have
developed applications for three specific use cases. The first to manage and
standardize the analysis of datasets from a large population-based cohort,
\gls{nowac}. The second to enable interactive exploration of these datasets.
Third to develop pipelines for analyzing sequencing datasets for use in a
precision medicine setting.  Although these use cases require widely different
systems with different requirements, the systems share common design patterns.
Figure \ref{overview-fig} shows the applications we have developed and the
underlying systems. 

\begin{figure}
\includegraphics[width=\textwidth]{figures/overall-arch.pdf}
    \caption{The applications and their underlying systems discussed in this
    thesis. } 
    \label{overview-fig}
\end{figure} 


We discuss the different areas separately before highlighting the similarities. 

\textbf{Data management and analysis}. 
Modern epidemiological studies integrate traditional questionnaire data with
information from public registries biological datasets.  These often span
multiple biological levels, i.e. different data types and collection sites.
While traditional questionnaire datasets require few specialized analysis tools
because of the relatively simple nature of the data, biological datasets require
specialized tools for reading, analyzing, and interpreting the data. Package
repositories such as Bioconductor provide a wealth of packages for analyzing
these datasets. These packages typically provide analysis tools, example data,
and comprehensive documentation. While the analysis code can be shared within
projects, the datasets are often stored on in-house databases or shared file
systems with specialized permissions, and referenced to in the analysis scripts.
We developed an approach for combining datasets from the \gls{nowac} cohort with
documentation, analysis scripts, and integrates with registry datasets. This
approach simplifies the task of starting analysis of the different data in our
study for new researchers.

% Si noe om access mgmt, brukere og data. 

Inspired by the ecosystem of packages in the R programming
language we developed our approach in the \gls{nowac} package. Users simply
install the package and get access to documentation, datasets, and utility
functions for analyzing datasets related to their area of research. 
In addition we version control both code and the data, making it possible to
track changes over time as the research study evolves. On top of the \gls{nowac}
package we implemented a pre-processing pipelining tool, Pippelinen. Pippelinen
is a web-based interface for running the standardized preprocessing steps before
analyzing gene expression datasets in the \gls{nowac} cohort. 

\textbf{Deep analysis pipelines}. 
Analysis of high-throughput sequencing datasets requires deep analysis pipelines
with a large number of steps that transform raw data into interpretable
results\cite{diao2015building}. There are a large number of tools available to
perform the different processing steps, written in a wide range of programming
languages. The tools, and their dependencies, can be difficult to install, and
they require users to correctly manage a range of input parameters that affects
the output results. With these observations in mind we used software containers
to package the tools we needed for our analyses, one tool per container image.
This made it possible to share the container image between compute systems
without installing any dependencies or additional packages. To keep track of
input parameters as well as the flow of data in a pipeline we designed a
text-based specification for analysis pipelines. This specification includes
information such as input parameters and tool versions. 

This approach was then implemented in walrus, a tool
that lets users create and run analysis pipelines. In addition, it tracks full
provenance of the input, intermediate, and output data, as well as tool
parameters. With \emph{walrus} we have successfully built analysis pipelines to
detect somatic mutations in breast cancer patients, as well as an \gls{rna}-seq
pipeline for comparison with gene expression datasets. 

\textbf{Interactive exploration}.
The final results from an analysis pipeline require researchers to investigate
and evaluate the final output. In addition, it may be useful to explore the
analysis parameters and re-run parts of the analyses. 
As with analysis pipelines, there are complete exploration tools as
well as software libraries to develop custom applications for exploration of
analysis results. The tools often require users to import already analyzed
datasets but provide interactive visualizations and point-and-click interfaces
to explore the data. Users with programming knowledge can use the wealth of
software packages for visualization within languages such as R or Python.
Modern visualization libraries created for the web brings the possibility to
develop applications that target users on any platform. From
these observations we wrote an interface to the R programming language, that
would allow us to interface with the wealth of existing software packages, e.g.
the \gls{nowac} package, for biological data analyses from a point-and-click
application. New data exploration applications could access analyses directly
through this interface, removing the previous decoupling between the two. 

This approach was then implemented as a part of \emph{Kvik}, a collection of
packages to develop new data exploration applications. Kvik allows
applications written in any modern programming language to interface with the
wealth of bioinformatics packages in the R programming language, as well as
information available through online databases.  To provide reproducible
execution environments we packaged these interfaces into  software containers
that can be easily deployed and shared.  We have used Kvik to develop the
\gls{mixt} system for exploring and comparing transcriptional profiles from
blood and tumor samples in breast cancer patients, in addition to applications
for exploring biological pathways. 

\textbf{Similarity}. 
The above approaches to build systems to analyze and explore biological data,
either as interactive applications or batch processing pipelines share the
same design principles. In all areas we break down the systems in to small
modular entities, e.g. a tool, and package these into software containers which
are then orchestrated together. These containers are configured and communicate
using open protocols that make it possible to interface with them using any
programming language. We can keep track of the configuration of the containers
and their orchestration using software versioning systems, and provide the
necessary information to reproduce analyses or a complete system. 

\section{Applications Implemented using Small Modular Entities} 
\begin{figure}
\includegraphics[width=\textwidth]{figures/overview-full.pdf}
    \caption{} 
    \label{overview-full}
\end{figure} 

In this section we detail the different systems we have built on top of walrus
and Kvik. 

The first system we built on top of walrus was a pipeline to analyze a patient’s
primary tumor and adjacent normal tissue, including subsequent metastatic
lesions.\cite{walrus} We packaged the necessary tools for the analyses into
software containers and wrote a pipeline description with all the necessary data
processing steps. Some of the steps required us to develop specialized scripts
to generate customized plots, but these were also wrapped in a container. From
the analyses we discovered, among other findings, inherited germline mutations
that are recognized to be among the top 50 mutations associated with an
increased risk of familial breast cancer. These were then shared with the
treating oncologists to aid the treatment plan. 

The second analysis pipeline we implemented was to enable comparison of a
\gls{rna}-seq dataset to microarray gene expression values collected from the
same samples.  The pipeline preprocesses the \gls{rna} dataset for all samples,
and generates transcript quantifications. As with the first pipeline we used
existing tools together with specialized analysis scripts packaged into a
container to ensure that we could reproduce the execution environments. 

% LAB: Hva med Pippelinen?
% TODO: PIPELINE AND NOWAC PAKGE

The first interactive data exploration application we built was Kvik Pathways.
It allows users to explore gene expression data from the \gls{nowac} cohort in
the context of interactive pathway maps.\cite{pathways} It is a web application
that integrates with the R programming language to provide an interface to the
statistical analyses. We used Kvik Pathways to repeat the analyses in a previous
published project that compared gene expression in blood from healthy women with
high and low plasma ratios of essential fatty acids.\cite{olsen2013plasma}

From the first application it became apparent that we could reuse parts of the
application in the implementation of later systems. In particular, the interface
to run analyses as well as the integration with the online databases could be
implemented as services, packaged into containers, and reused in the next
application we developed. Both of these were designed and implemented in Kvik,
which could then be used and shared later. 

The second application we built the \gls{mixt} web application. A system to
explore and compare transcriptional profiles from blood and tumor samples in
breast cancer patients. The application is built to simplify the exploration of
results from the Matched Interactions Across Tissues (MIxT) study. Its goal was
to identify genes and pathways in the primary breast tumor that are tightly
linked to genes and pathways in the patient blood
cells.\cite{dumeaux2017interactions} The web application interfaces with the
methods implemented as an R package and integrates the results together with
information from biological databases through a simple user interface. 

A third application we developed was a simple re-deployment of the \gls{mixt}
web application with a new dataset. In this application we simply replaced the R
package with a new package that interfaced with different data. All the other
components are reused and highlights the flexibility of the approach. 

Combined these systems and applications demonstrate how small modular entities
are useful for both batch processing of datasets, as well as interactive
applications. 

\section{Summary of Results} 
% TODO: SCREENSHOTS OF SOME OF THE APPS 
% TODO: Pipeline/NOWAC PKG
% TODO: Reproducibility in walrus results. 
% TODO: LAB: jeg savner "easy/ quickly" biten
We show the viability of our approach through real-world applications in systems
epidemiology and precision medicine. We demonstrate its usefulness for building
interactive data exploration application, implemented in Kvik. We show the
applicability of small modular entities in deep analysis pipelines, as
implemented in walrus.

We have used walrus to analyze a whole-exome dataset to from a sample in the
McGill Genome Quebec [MGGQ] dataset (GSE58644)\cite{tofigh2014prognostic} to
discover \glspl{snp}, genomic variants and somatic mutations. Using walrus to
analyze a dataset added 10\% to the runtime and doubled the space requirements,
but reduced days of compute time down to seconds when restoring a previous
pipeline configuration. 

We have used the packages in Kvik to develop a web application, MIxT
blood-tumor, for exploring and comparing transcriptional profiles from blood and
tumor samples in breast cancer patients.  In addition we have used it to build
an application to explore gene expression data in the context of biological
pathways. We show that developing an application using a microservice approach
allows us to reduce database query times down to 90\%, and that we can provide
an interface to statistical analyses that is up to 10 times as fast as
alternative approaches. 

Together the results show that our approach, small modular entities, can be used
to enable reproducible data analysis and exploration of high-throughput
biological datasets while still providing the required performance. 

\section{List of papers} 
% TODO: wording of contributions (I ...)
% TODO: number the papers!!!! 

This section contains a list of papers along with short descriptions and my
contributions to each paper. 
\capstartfalse
\begin{table}[H]
    \centering
    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & Kvik: three-tier data exploration tools for flexible analysis
         of genomic data in epidemiological studies \\ \hline
         
         Authors & \textbf{Bjørn Fjukstad}, Karina Standahl Olsen, Mie Jareid,
         Eiliv Lund, and Lars Ailo Bongo \\ \hline
         
         Description & The initial description of Kvik, and how we used it to
         implement Kvik Pathways, a web application for browsing biologicap
         pathway maps integrated with gene expression data from the \gls{nowac}
         cohort. 
         \\ \hline
         
         Contribution & I designed, implemented, and deployed Kvik and Kvik
         Pathways. Evaluated the system and wrote the manuscript. \\ \hline
         
         Publication date & 15 March 2015 \\ \hline 

         Publication venue & F1000 \\ \hline
         
         Citation & \cite{fjukstad2015kvik} \bibentry{fjukstad2015kvik} \\
         \hline 
    \end{tabular}
    \label{p1}

\end{table}
% \hfill 
\begin{table}[H]

    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & Building Applications For Interactive Data Exploration In
         Systems Biology. \\ \hline
         
         Authors & \textbf{Bjørn Fjukstad}, Vanessa Dumeaux, Karina
         Standahl Olsen, Michael Hallett, Eiliv Lund, and Lars Ailo Bongo.  \\
         \hline
         
         Description & Describes how we further developed the ideas from Paper 1
         into an approach that we used to build the \gls{mixt} web application. 
         \\ \hline
         
         Contribution & 
         Designed, implemented, and deployed Kvik and the \gls{mixt} web
         application.  Evaluated the system and wrote the manuscript. 
         \\ \hline
         
         Publication date & 20 August 2017. \\ \hline  

         Publication venue & The 8th ACM Conference on Bioinformatics,
         Computational Biology, and Health Informatics (ACM BCB) August 20–23,
         2017.  \\
         \hline
         
         Citation & \cite{fjukstad2017building} \bibentry{fjukstad2017building}
         \\ \hline 
    \end{tabular}
    \label{p2}
    
\end{table}
% \hfill 
\begin{table}[H]
    
    \centering
    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & Interactions Between the Tumor and the Blood Systemic Response
         of Breast Cancer Patients \\ \hline
         
         Authors & Vanessa Dumeaux, \textbf{Bjørn Fjukstad}, Hans E Fjosne,
         Jan-Ole Frantzen, Marit Muri Holmen, Enno Rodegerdts, Ellen
         Schlichting, Anne-Lise Børresen-Dale, Lars Ailo Bongo, Eiliv Lund,
         Michael Hallett.  \\ \hline
         
         Description & Describes the \gls{mixt} system which enables
         identification of genes and pathways in the primary tumor that are
         tightly
         linked to genes and pathways in the patient \gls{sr}. 
         \\ \hline
         
         Contribution & 
         Designed, implemented, and deployed the \gls{mixt} web application.
        Contributed to write the manuscript. 
         \\ \hline
         
         Publication date & 28 September 2017. \\ \hline  

         Publication venue &  PLoS Computational Biology \\ \hline
         
         Citation & \cite{dumeaux2017interactions}
         \bibentry{dumeaux2017interactions}
         \\ \hline 
    \end{tabular}
    \label{p3}
    
    \hfill 

    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & A Review of Scalable Bioinformatics Pipelines \\ \hline
         
         Authors & \textbf{Bjørn Fjukstad}, Lars Ailo Bongo. \\ \hline
         
         Description & This review survey several scalable bioinformatics
         pipelines and compare their design and their use of underlying
         frameworks and infrastructures.      \\ \hline
         
         Contribution & Literature review and Wrote the manuscript.  \\ \hline
         
         Publication date & 23 October 2017 \\ \hline  

         Publication venue & Data Science and Engineering 2017. \\ \hline
         
         Citation & \cite{fjukstad2017review} \bibentry{fjukstad2017review} \\
         \hline 
    \end{tabular}
    \label{p4}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & nsroot: Minimalist Process Isolation Tool Implemented With
         Linux Namespaces.  \\ \hline
         
         Authors & Inge Alexander Raknes, \textbf{Bjørn Fjukstad}, Lars Ailo
         Bongo. \\ \hline
         
         Description & Describes a tool for process isolation built using Linux
         namespaces.          \\ \hline
         
         Contribution & Contributed to the
         manuscript, specifically to the literature review and related works.
         \\ \hline
         
         Publication date & 26 November 2017 \\ \hline  

         Publication venue & Norsk Informatikkonferanse 2017. \\ \hline
         
         Citation & \cite{fjukstad2017review} \bibentry{fjukstad2017review} \\
         \hline 
    \end{tabular}
    \label{p5}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & Transcription factor PAX6 as a novel prognostic factor and
         putative tumour suppressor in non-small cell lung cancer \\ \hline
         
         Authors & Yury Kiselev, Sigve Andersen, Charles Johannessen, 
         \textbf{Bjørn Fjukstad}, Karina Standahl Olsen, Helge Stenvold, Samer
         Al-Saad, Tom Dønnem, Elin Richardsen, Roy M Bremnes, and Lill-Tove
         Rasmussen Busund.\\ \hline
         
         Description & This paper explores the possibility of using the PAX6
         transcription factor as a prognostic marker in non-small cell lung
         cancer. 
         \\ \hline
         
         Contribution & Did the analyses to explore association between PAX6
         gene expression and PAX6 target genes. 
         \\ \hline
         
         Publication date & 22 March 2018 \\ \hline  

         Publication venue & Scientific Reports 2018. \\ \hline
         
         Citation & \cite{kiselev2018transcription}
         \bibentry{kiselev2018transcription} \\
         \hline 
    \end{tabular}
    \label{p6}
\end{table}


\begin{table}[H]

    \centering
    \begin{tabular}{ | l | p{9.5cm} | }
    \hline
         Title & Reproducible Data Analysis Pipelines in Precision Medicine \\
         \hline
         
         Authors &  \textbf{Bjørn Fjukstad}, Vanessa Dumeaux, Michael Hallett,
         Lars Ailo Bongo\\ \hline
         
         Description & This paper outlines how we used the container centric
         development model to build walrus. 
         \\ \hline
         
         Contribution & Design, implementation and evaluation of walrus. Wrote
         the manuscript. 
         \\ \hline
         
         Publication date & TBA \\ \hline  

         Publication venue & TBA \\ \hline
         
         Citation & \cite{walrus} \bibentry{walrus} \\
         \hline 
    \end{tabular}
    \label{p6}
\end{table}

% Husk historien: Fra rå data gjennom komplekse analyse pipeliner og helt til
% forskere kan svømme rundt i resultatene. 

\section{Dissertation Plan} 
This thesis is organized as follows. Chapter 2 describes the characteristics of
state-of-the-art biological datasets in systems epidemiology and how we have
developed an approach to analyze these. 
In Chapter 3 we describe how we used the same ideas and model to develop
applications for interactively exploring results from statistical analyses.
Chapter 4 explores how we can develop analysis pipelines for high-throughput
sequencing datasets in precision medicine. It describes in detail how we use a
container centric development model to build a tool, walrus, to develop and
execute these pipelines.
Finally, Chapter 5 concludes the work and discusses future directions. 

